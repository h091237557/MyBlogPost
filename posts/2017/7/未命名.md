在分析前，我先建立以下四個假設 (不然可能性過多)

1. 假設該資料表為存放每一位使用者在觀看某商品的 log。
2. 假設它存放的欄位有 id、product_id、user_id、time。
3. 大部份的操作都為寫，比較少讀的工作。
4. 假設讀的需求中，操作最多的為尋找某段時間熱門的產品。

案例有說到它是在單一表上，數據量會增加，所以我會準備根據『某個欄位』進行資料表切分，根據上面的假設3，因為我們大部份的工作都為寫，所以我們的目的是『將寫的操作流量平均的分散的不同的表，並且可以讓我們最快速的尋找到一段時間的熱門產品』。

訂好目標時，就來決定要用那個欄位來分配資料，首先來看將寫的操作分散這目的，首先如果直接用 time 一定不行，因為如果用了它，就會發生所有的寫操作都會往某個表一直塞，所以接下來比較好的選擇，是那三個 id。

這三個 id 的編碼假設如果是非時間序列的話，那就都可以選擇，反之則要另外在討論，先假設它們為非時間序列的資料，接下來我們在看另一個目的『一段時間的熱門產品』，基本上看到這個，我們就可以直接選用 product_id 來當片鍵，因為這樣我們在尋找熱門的產品，只要判斷他的product_id屬於那張表，然後就直接在去那張表進行 query ，就很方便了。


前端 : 由前端來決定，顯示畫面、分頁和個數。

端點 : 這邊會建立一個 api :Get http://xxxx/product 來讓前端取得 products，並且要代參數，來決定取得的分頁與數量。

資料表 : orders、view_logs、products

程式方面 : 每一次我們的前端呼叫請求時，我們需要先去view_logs取得這近 7 天，有被觀看的商品，並且計算出每個產品被觀看的個數，而同時也要去 orders 取得前 7 天的訂單，並且計算出每個產品買的人數，然後再計算出它的轉換率，然後可以在使用該結果去 products 取得 product 回傳給前端。

這邊上有幾點要注意，這種大筆資料的搜尋第一個原則是，儘可能的先將筆數減少，再來計算，而且如果計算時間太長，請開 child_process，並且把結果存放在 cache 中 (redies)，每次請求先去 cache 取得，然後一定時間後，在取 db 裡更新。